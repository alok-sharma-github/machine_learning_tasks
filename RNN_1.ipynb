{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2JTVj6Cpcue8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZbOAL-Gc1l1",
        "outputId": "7f6dc452-44a8-4703-be1b-0efc1357e6f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "\n",
        "vocab_size = 10000\n",
        "max_len = 100\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tioCYl3-c4IW"
      },
      "outputs": [],
      "source": [
        "# pad sequences to ensure uniform length\n",
        "x_train = pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = pad_sequences(x_test, maxlen=max_len)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnONA-DJe71d",
        "outputId": "a572bc11-383b-450e-befd-5c69de6f4954"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y_train (25000,)\n",
            "x_train (25000, 100)\n",
            "x_test (25000, 100)\n",
            "y_test (25000,)\n"
          ]
        }
      ],
      "source": [
        "print(\"y_train\",y_train.shape)\n",
        "print(\"x_train\",x_train.shape)\n",
        "print(\"x_test\",x_test.shape)\n",
        "print(\"y_test\",y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXZLVhhDc6M6",
        "outputId": "4db168d5-490f-422d-8f23-795ed5664286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 100, 32)           320000    \n",
            "                                                                 \n",
            " simple_rnn_1 (SimpleRNN)    (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 322113 (1.23 MB)\n",
            "Trainable params: 322113 (1.23 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Define the RNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=32, input_length=max_len))\n",
        "model.add(SimpleRNN(units=32, return_sequences = False))  # SimpleRNN layer with 32 units\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YD7NvAMuc86A",
        "outputId": "a8e528bb-8f7b-4232-d5f9-88f48f4bd7d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "157/157 [==============================] - 34s 201ms/step - loss: 0.6740 - accuracy: 0.5687 - val_loss: 0.5310 - val_accuracy: 0.7554\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 23s 144ms/step - loss: 0.4112 - accuracy: 0.8190 - val_loss: 0.4155 - val_accuracy: 0.8160\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 18s 113ms/step - loss: 0.2283 - accuracy: 0.9149 - val_loss: 0.4257 - val_accuracy: 0.8328\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 17s 107ms/step - loss: 0.1047 - accuracy: 0.9672 - val_loss: 0.4965 - val_accuracy: 0.8058\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 15s 94ms/step - loss: 0.0430 - accuracy: 0.9907 - val_loss: 0.5507 - val_accuracy: 0.8102\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 0.5676 - accuracy: 0.7973\n",
            "Test Loss: 0.5676332712173462\n",
            "Test Accuracy: 0.7973200082778931\n"
          ]
        }
      ],
      "source": [
        "model.fit(x_train, y_train, epochs=5,batch_size=128, validation_split=0.2)\n",
        "\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C7Y-RILdWie",
        "outputId": "bc0dd0e2-531e-4454-8f17-703984641f65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 7s 9ms/step\n"
          ]
        }
      ],
      "source": [
        "y_pred_probs = model.predict(x_test)\n",
        "y_pred = (y_pred_probs > 0.5).astype(np.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAWQiDuYLLn0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZcP_FJEfwYI",
        "outputId": "a49fc4a2-ebe7-4d7e-c9f6-435cd7c06a7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(25000, 1)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If9hDumCk_ZJ",
        "outputId": "7bf744da-2e9c-452f-aaa0-e562f3b8c7f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,\n",
              "        591,  202,   14,   31,    6,  717,   10,   10,    2,    2,    5,\n",
              "          4,  360,    7,    4,  177, 5760,  394,  354,    4,  123,    9,\n",
              "       1035, 1035, 1035,   10,   10,   13,   92,  124,   89,  488, 7944,\n",
              "        100,   28, 1668,   14,   31,   23,   27, 7479,   29,  220,  468,\n",
              "          8,  124,   14,  286,  170,    8,  157,   46,    5,   27,  239,\n",
              "         16,  179,    2,   38,   32,   25, 7944,  451,  202,   14,    6,\n",
              "        717], dtype=int32)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_test[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgS48oxgoIii"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYXi3_g8k5g2",
        "outputId": "75cb0005-b79f-4535-dcd9-d319f61caa6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n",
            "Probability for the new sentence:\n",
            "[[0.00262302]]\n",
            "Sentiment [[0]]\n"
          ]
        }
      ],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyPiGVuJtdZy",
        "outputId": "d2c28a4d-ad95-4638-f70a-41c92d307d25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoded text:\n",
            "in as a br passion yet\n",
            "Padded sequence:\n",
            "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0   11   17    6   10 1797    2    1\n",
            "   246    2]]\n",
            "New sequence:\n",
            "[[  11   17    6   10 1797    2    1  246    2]]\n"
          ]
        }
      ],
      "source": [
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "def decode_sequence(sequence):\n",
        "    decoded_words = []\n",
        "    for idx in sequence:\n",
        "        word = reverse_word_index.get(idx - 3, '?')  # Adjust for word index offset\n",
        "        if word != '?':  # Skip padding tokens\n",
        "            decoded_words.append(word)\n",
        "    return ' '.join(decoded_words)\n",
        "\n",
        "\n",
        "new_sentence = \"This movie is I hated it, the worst movie.\"\n",
        "# Tokenize the new sentence\n",
        "new_sequence = [word_index.get(word, 2) for word in new_sentence.lower().split()]  # Use lowercase and handle unknown words\n",
        "new_sequence = np.array([new_sequence])\n",
        "\n",
        "# Pad the tokenized sequence\n",
        "padded_sequence = pad_sequences(new_sequence, maxlen=max_len)\n",
        "\n",
        "# Decode the padded sequence\n",
        "decoded_texts = [decode_sequence(seq) for seq in padded_sequence]\n",
        "\n",
        "# Print the decoded text\n",
        "print(\"Decoded text:\")\n",
        "for text in decoded_texts:\n",
        "    print(text)\n",
        "\n",
        "# Print the sequences\n",
        "print(\"Padded sequence:\")\n",
        "print(padded_sequence)\n",
        "print(\"New sequence:\")\n",
        "print(new_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "aSCYOyWbu26B",
        "outputId": "0fde282d-c4e1-4b7e-fa9c-c176fb800f34"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'decoded_texts' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c5f39a342baf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'decoded_texts' is not defined"
          ]
        }
      ],
      "source": [
        "print(decoded_texts)\n",
        "print(padded_sequence)\n",
        "print(new_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6V-0rEOuLBi"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenize = Tokenizer()\n",
        "new_sentence = \"This movie is boring. I hated it.\"\n",
        "new_sequence = tokenize.texts_to_sequences([new_sentence])\n",
        "padded_sequence = pad_sequences(new_sequence, maxlen=max_len)\n",
        "predictions = model.predict(padded_sequence)\n",
        "print(\"Probability for the new sentence:\")\n",
        "print(predictions)\n",
        "y_pred = (predictions > 0.5).astype(np.int64)\n",
        "print(\"Sentiment\",y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb932VZBOZrQ",
        "outputId": "16ad96a1-33fc-4552-e244-2b770fe1c377"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 100, 32)           320000    \n",
            "                                                                 \n",
            " simple_rnn_2 (SimpleRNN)    (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 322113 (1.23 MB)\n",
            "Trainable params: 322113 (1.23 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "157/157 [==============================] - 25s 153ms/step - loss: 0.5813 - accuracy: 0.6651 - val_loss: 0.4660 - val_accuracy: 0.7808\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 20s 124ms/step - loss: 0.3266 - accuracy: 0.8658 - val_loss: 0.3945 - val_accuracy: 0.8316\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 16s 102ms/step - loss: 0.2175 - accuracy: 0.9188 - val_loss: 0.4073 - val_accuracy: 0.8398\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 17s 111ms/step - loss: 0.1321 - accuracy: 0.9563 - val_loss: 0.4446 - val_accuracy: 0.8352\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 15s 97ms/step - loss: 0.0725 - accuracy: 0.9792 - val_loss: 0.5322 - val_accuracy: 0.8212\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 0.5536 - accuracy: 0.8147\n",
            "Test Loss: 0.553577721118927\n",
            "Test Accuracy: 0.8146799802780151\n",
            "782/782 [==============================] - 7s 9ms/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n",
            "Decoded text:\n",
            "in as a br passion yet\n",
            "Padded sequence:\n",
            "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0   11   17    6   10 1797    2    1\n",
            "   246    2]]\n",
            "New sequence:\n",
            "[[  11   17    6   10 1797    2    1  246    2]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the IMDb dataset\n",
        "vocab_size = 10000\n",
        "max_len = 100\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "# pad sequences to ensure uniform length\n",
        "x_train = pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = pad_sequences(x_test, maxlen=max_len)\n",
        "\n",
        "# Define the RNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=32, input_length=max_len))\n",
        "model.add(SimpleRNN(units=32, return_sequences=False))  # SimpleRNN layer with 32 units\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=128, validation_split=0.2)\n",
        "\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "y_pred_probs = model.predict(x_test)\n",
        "y_pred = (y_pred_probs > 0.5).astype(np.int64)\n",
        "\n",
        "# Decode function to convert sequences back to text\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7PuAW-OOaOY",
        "outputId": "c06bbc14-6c5a-45e6-8cd2-c231ca0e5848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoded text:\n",
            "this movie is i hated and the worst and\n",
            "Padded sequence:\n",
            "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0   11   17    6   10 1797    2    1\n",
            "   246    2]]\n",
            "New sequence:\n",
            "[[  11   17    6   10 1797    2    1  246    2]]\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk4hySUXVrDr",
        "outputId": "ec6f6c28-7f7b-4f90-c924-c1ee898fbd1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 3s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 2s 1us/step\n",
            "                                              review  sentiment\n",
            "0  ? this film was just brilliant casting locatio...          1\n",
            "1  ? big hair big boobs bad music and a giant saf...          0\n",
            "2  ? this has to be one of the worst films of the...          0\n",
            "3  ? the ? ? at storytelling the traditional sort...          1\n",
            "4  ? worst mistake of my life br br i picked this...          0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# Load the IMDb dataset\n",
        "vocab_size = 10000\n",
        "max_len = 100\n",
        "(x_train, y_train), (_, _) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "# Decode the integer sequences back to text\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = {value: key for key, value in word_index.items()}\n",
        "\n",
        "def decode_sequence(sequence):\n",
        "    return ' '.join([reverse_word_index.get(idx - 3, '?') for idx in sequence])\n",
        "\n",
        "# Decode sequences and create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'review': [decode_sequence(seq) for seq in x_train],\n",
        "    'sentiment': y_train\n",
        "})\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"? the ? ? at storytelling the traditional sort many years after the event i can still see in my ? eye an elderly lady my friend's mother retelling the battle of ? she makes the characters come alive her passion is that of an eye witness one to the events on the ? heath a mile or so from where she lives br br of course it happened many years before she was born but you wouldn't guess from the way she tells it the same story is told in bars the length and ? of scotland as i discussed it with a friend one night in ? a local cut in to give his version the discussion continued to closing time br br stories passed down like this become part of our being who doesn't remember the stories our parents told us when we were children they become our invisible world and as we grow older they maybe still serve as inspiration or as an emotional ? fact and fiction blend with ? role models warning stories ? magic and mystery br br my name is ? like my grandfather and his grandfather before him our protagonist introduces himself to us and also introduces the story that stretches back through generations it produces stories within stories stories that evoke the ? wonder of scotland its rugged mountains ? in ? the stuff of legend yet ? is ? in reality this is what gives it its special charm it has a rough beauty and authenticity ? with some of the finest ? singing you will ever hear br br ? ? visits his grandfather in hospital shortly before his death he burns with frustration part of him ? to be in the twenty first century to hang out in ? but he is raised on the western ? among a ? speaking community br br yet there is a deeper conflict within him he ? to know the truth the truth behind his ? ancient stories where does fiction end and he wants to know the truth behind the death of his parents br br he is pulled to make a last ? journey to the ? of one of ? most ? mountains can the truth be told or is it all in stories br br in this story about stories we ? bloody battles ? lovers the ? of old and the sometimes more ? ? of accepted truth in doing so we each connect with ? as he lives the story of his own life br br ? the ? ? is probably the most honest ? and genuinely beautiful film of scotland ever made like ? i got slightly annoyed with the ? of hanging stories on more stories but also like ? i ? this once i saw the ? picture ' forget the box office ? of braveheart and its like you might even ? the ? famous ? of the wicker man to see a film that is true to scotland this one is probably unique if you maybe ? on it deeply enough you might even re ? the power of storytelling and the age old question of whether there are some truths that cannot be told but only experienced\""
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['review'][]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSH237FEWQcs",
        "outputId": "89addbd2-7c49-46f4-8858-809af40b5aea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoded text:\n",
            "cry at a film it must have been good and this definitely was also and to the two little boy's that played the and of norman and paul they were just brilliant children are often left out of the and list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
            "Padded sequence:\n",
            "[[1412   30    3   19    9  212   25   74   49    2   11  404   13   79\n",
            "     2    5    1  104  114 5949   12  253    1    2    4 3763    2  720\n",
            "    33   68   40  527  473   23  397  314   43    4    1    2 1026   10\n",
            "   101   85    1  378   12  294   95   29 2068   53   23  138    3  191\n",
            "  7483   15    1  223   19   18  131  473   23  477    2  141   27 5532\n",
            "    15   48   33   25  221   89   22  101    1  223   62   13   35 1331\n",
            "    85    9   13  280    2   13 4469  110  100   29   12   13 5342   16\n",
            "   175   29]]\n",
            "New sequence:\n",
            "[[   2   11   19   13   40  527  970 1619 1382   62  455 4465   63 3938\n",
            "     1  170   33  253    2   22   97   40  835  109   47  667    2    6\n",
            "    32  477  281    2  147    1  169  109  164    2  333  382   36    1\n",
            "   169 4533 1108   14  543   35   10  444    1  189   47   13    3  144\n",
            "  2022   16   11   19    1 1917 4610  466    1   19   68   84    9   13\n",
            "    40  527   35   73   12   10 1244    1   19   14  512   14    9   13\n",
            "   623   15    2    2   59  383    9    5  313    5  103    2    1 2220\n",
            "  5241   13  477   63 3782   30    1  127    9   13   35  616    2   22\n",
            "   121   48   33  132   45   22 1412   30    3   19    9  212   25   74\n",
            "    49    2   11  404   13   79    2    5    1  104  114 5949   12  253\n",
            "     1    2    4 3763    2  720   33   68   40  527  473   23  397  314\n",
            "    43    4    1    2 1026   10  101   85    1  378   12  294   95   29\n",
            "  2068   53   23  138    3  191 7483   15    1  223   19   18  131  473\n",
            "    23  477    2  141   27 5532   15   48   33   25  221   89   22  101\n",
            "     1  223   62   13   35 1331   85    9   13  280    2   13 4469  110\n",
            "   100   29   12   13 5342   16  175   29]]\n"
          ]
        }
      ],
      "source": [
        "def decode_sequence(sequence):\n",
        "    decoded_words = []\n",
        "    for idx in sequence:\n",
        "        word = reverse_word_index.get(idx, '?')  # Get the word directly from the reverse word index\n",
        "        if word != '?' and word != '<PAD>':  # Skip unknown and padding tokens\n",
        "            decoded_words.append(word)\n",
        "    return ' '.join(decoded_words)\n",
        "\n",
        "# New sentence to test\n",
        "new_sentence = df['review'][0]\n",
        "# Tokenize the new sentence\n",
        "new_sequence = [word_index.get(word, 2) for word in new_sentence.lower().split()]  # Use lowercase and handle unknown words\n",
        "new_sequence = np.array([new_sequence])\n",
        "\n",
        "# Pad the tokenized sequence\n",
        "padded_sequence = pad_sequences(new_sequence, maxlen=max_len)\n",
        "\n",
        "# Decode the padded sequence\n",
        "decoded_texts = [decode_sequence(seq) for seq in padded_sequence]\n",
        "\n",
        "# Print the decoded text\n",
        "print(\"Decoded text:\")\n",
        "for text in decoded_texts:\n",
        "    print(text)\n",
        "\n",
        "# Print the sequences\n",
        "print(\"Padded sequence:\")\n",
        "print(padded_sequence)\n",
        "print(\"New sequence:\")\n",
        "print(new_sequence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aFVBQofahbx",
        "outputId": "7e16c120-00a1-40f5-a68e-b939c5d8b87c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n",
            "Probability for the new sentence:\n",
            "[[0.88484263]]\n",
            "Sentiment [[1]]\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(padded_sequence)\n",
        "print(\"Probability for the new sentence:\")\n",
        "print(predictions)\n",
        "y_pred = (predictions > 0.5).astype(np.int64)\n",
        "print(\"Sentiment\",y_pred)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
