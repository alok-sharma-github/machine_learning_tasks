{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# DQN (Deep Q-Network) class definition\n",
    "class DQN(models.Model):\n",
    "    def __init__(self, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # Define the layers for the neural network\n",
    "        self.dense1 = layers.Dense(32, activation='relu', input_shape=(84, 84, 4))\n",
    "        self.dense2 = layers.Dense(64, activation='relu')\n",
    "        self.dense3 = layers.Dense(action_size, activation='linear')\n",
    "        self.flatten = layers.Flatten()\n",
    "\n",
    "    # Feed forward network  \n",
    "    def call(self, state):\n",
    "        # Flatten the state\n",
    "        s = self.flatten(state)\n",
    "        # Pass through the first dense layer\n",
    "        s = self.dense1(s)\n",
    "        # Pass through the second dense layer\n",
    "        s = self.dense2(s)\n",
    "        # Output layer with linear activation\n",
    "        return self.dense3(s)\n",
    "\n",
    "# Agent class definition\n",
    "class DQNagent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Initialize state and action sizes\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        # Initialize a deque (double-ended queue) for experience replay\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        # Initialize exploration rate (epsilon) to encourage exploration\n",
    "        self.epsilon = 1.0\n",
    "        # Minimum exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        # Decay rate for exploration rate\n",
    "        self.epsilon_decay = 0.995\n",
    "        # Initialize discount factor (gamma) for future rewards\n",
    "        self.gamma = 0.95\n",
    "        # Build the DQN model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    # Method to build the DQN model\n",
    "    def build_model(self):\n",
    "        # Create an instance of the DQN class\n",
    "        model = DQN(self.action_size)\n",
    "        # Compile the model with Adam optimizer and Mean Squared Error loss\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        # Return the compiled model\n",
    "        return model\n",
    "\n",
    "    # Method to store experience in memory buffer\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # Method to select action using epsilon-greedy policy\n",
    "    def act(self, state):\n",
    "        # Epsilon-greedy policy\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # Explore: Choose a random action\n",
    "            return random.randrange(self.action_size)\n",
    "        # Exploit: Choose the action with highest Q-value from the DQN model\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    # Method to train the DQN model\n",
    "    def replay(self, batch_size):\n",
    "        # Check if the memory buffer size is sufficient for sampling\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample a batch of experiences from memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # If the episode is done, set target to reward\n",
    "            target = reward\n",
    "            if not done:\n",
    "                # Calculate target using Bellman equation\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "            # Get current Q-values from the DQN model\n",
    "            target_f = self.model.predict(state)\n",
    "            # Update the Q-value for the chosen action\n",
    "            target_f[0][action] = target\n",
    "            # Train the DQN model using the updated target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        # Decay epsilon (exploration rate) over time\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unexpected number of values returned by env.step()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m     next_state, reward, done, info \u001b[38;5;241m=\u001b[39m step_result\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected number of values returned by env.step()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Store the experience in the agent's memory\u001b[39;00m\n\u001b[0;32m     37\u001b[0m agent\u001b[38;5;241m.\u001b[39mremember(state, action, reward, next_state, done)\n",
      "\u001b[1;31mValueError\u001b[0m: Unexpected number of values returned by env.step()"
     ]
    }
   ],
   "source": [
    "# Create the CartPole environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Number of episodes to run\n",
    "episodes = 10\n",
    "\n",
    "# Initialize the DQN agent\n",
    "agent = DQNagent(state_size=4, action_size=2)\n",
    "\n",
    "# Loop through each episode\n",
    "for epi in range(1, episodes + 1):\n",
    "    # Reset the environment to start a new episode\n",
    "    state = env.reset()\n",
    "    done = False  # Whether the episode has ended or not\n",
    "    score = 0  # Initialize the score for this episode\n",
    "\n",
    "    # Run the episode until it's done\n",
    "    while not done:\n",
    "        # Choose an action for the agent using epsilon-greedy policy\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # Take a step in the environment based on the chosen action\n",
    "        step_result = env.step(action)\n",
    "        \n",
    "        # Check the length of the returned tuple\n",
    "        if len(step_result) == 2:\n",
    "            next_state, reward = step_result\n",
    "            done = False  # Assuming episode is not done if only next_state and reward are returned\n",
    "        elif len(step_result) == 3:\n",
    "            next_state, reward, done = step_result\n",
    "        elif len(step_result) == 4:\n",
    "            next_state, reward, done, info = step_result\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected number of values returned by env.step()\")\n",
    "\n",
    "        # Store the experience in the agent's memory\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        # Accumulate the reward to compute the score for this episode\n",
    "        score += reward\n",
    "\n",
    "        # Update the current state to the next state\n",
    "        state = next_state\n",
    "\n",
    "    # Train the agent using experiences from memory\n",
    "    agent.replay(batch_size=32)\n",
    "\n",
    "    # Render the environment\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Episodes:{epi}, Score: {score}\")\n",
    "    plt.show()\n",
    "    clear_output(wait=True)\n",
    "\n",
    "# Close the rendered frames\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
