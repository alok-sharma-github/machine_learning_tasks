{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "(train_images, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5  # Normalize to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up discriminator\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=(28, 28, 1)),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "discriminator = make_discriminator_model()\n",
    "discriminator.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up generator\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(7*7*256, use_bias=False, input_shape=(100,)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Reshape((7, 7, 256)),\n",
    "        layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "generator = make_generator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GAN\n",
    "def make_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    gan = tf.keras.Sequential([generator, discriminator])\n",
    "    gan.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='binary_crossentropy')\n",
    "    return gan\n",
    "\n",
    "gan = make_gan(generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 224\n  y sizes: 256\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 36>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(train_images)\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m60000\u001b[39m)\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Train the GAN\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mtrain_gan\u001b[1;34m(gan, generator, discriminator, dataset, epochs, batch_size)\u001b[0m\n\u001b[0;32m     18\u001b[0m labels \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(tf\u001b[38;5;241m.\u001b[39mshape(labels))\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Train discriminator\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Train generator\u001b[39;00m\n\u001b[0;32m     24\u001b[0m noise \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal([batch_size, \u001b[38;5;241m100\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\alok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:2474\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[0;32m   2471\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope(), training_utils\u001b[38;5;241m.\u001b[39mRespectCompiledTrainableState(  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m   2472\u001b[0m     \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   2473\u001b[0m ):\n\u001b[1;32m-> 2474\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingle_batch_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2475\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight\u001b[49m\n\u001b[0;32m   2476\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[0;32m   2478\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n",
      "File \u001b[1;32mc:\\Users\\alok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py:1828\u001b[0m, in \u001b[0;36msingle_batch_iterator\u001b[1;34m(strategy, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1825\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1826\u001b[0m     data \u001b[38;5;241m=\u001b[39m (x, y, sample_weight)\n\u001b[1;32m-> 1828\u001b[0m \u001b[43m_check_data_cardinality\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1829\u001b[0m dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensors(data)\n\u001b[0;32m   1830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_weight:\n",
      "File \u001b[1;32mc:\\Users\\alok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py:1848\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1841\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1842\u001b[0m         label,\n\u001b[0;32m   1843\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m   1844\u001b[0m             \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[0;32m   1845\u001b[0m         ),\n\u001b[0;32m   1846\u001b[0m     )\n\u001b[0;32m   1847\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1848\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 224\n  y sizes: 256\nMake sure all arrays contain the same number of samples."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "def train_gan(gan, generator, discriminator, dataset, epochs=5 , batch_size=128):\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataset:\n",
    "            # Generate noise\n",
    "            noise = tf.random.normal([batch_size, 100])\n",
    "\n",
    "            # Generate fake images\n",
    "            generated_images = generator(noise)\n",
    "\n",
    "            # Combine real and fake images\n",
    "            combined_images = tf.concat([generated_images, batch], axis=0)\n",
    "\n",
    "            # Labels for real and fake images\n",
    "            labels = tf.concat([tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0)\n",
    "\n",
    "            # Add noise to labels\n",
    "            labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "            # Train discriminator\n",
    "            d_loss = discriminator.train_on_batch(combined_images, labels)\n",
    "\n",
    "            # Train generator\n",
    "            noise = tf.random.normal([batch_size, 100])\n",
    "            misleading_labels = tf.zeros((batch_size, 1))\n",
    "            g_loss = gan.train_on_batch(noise, misleading_labels)\n",
    "\n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Discriminator Loss: {d_loss}, Generator Loss: {g_loss}\")\n",
    "\n",
    "# Convert training images to TensorFlow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(60000).batch(128)\n",
    "\n",
    "# Train the GAN\n",
    "train_gan(gan, generator, discriminator, train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
