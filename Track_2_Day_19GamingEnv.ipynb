{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAP7FzGnhUet"
      },
      "source": [
        "SNAKE GAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "enNdhqaWg9db"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pygame\n",
        "import time\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- GREEN, RED, and BLACK: These are RGB color tuples used to define the colors of various elements in the game. Specifically:\n",
        "- GREEN is used to represent the color of the snake's body.\n",
        "- RED is used to represent the color of the food that the snake eats.\n",
        "- BLACK is used to represent the color of the background or empty spaces in the game grid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SnakeGame Class:\n",
        "\n",
        "- This class represents the game environment.\n",
        "- It initializes the game state, including the snake's initial position, direction, and food location.\n",
        "- The move method updates the game state based on the agent's action (UP, DOWN, LEFT, RIGHT).\n",
        "- The get_state method returns the current state of the game as a grid with the snake and food positions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class SnakeGame:\n",
        "#     def __init__(self):\n",
        "#         self.width = GRID_WIDTH\n",
        "#         self.height = GRID_HEIGHT\n",
        "#         self.reset()\n",
        "\n",
        "#     def reset(self):\n",
        "#         self.snake = [(self.width // 2, self.height // 2)]\n",
        "#         self.direction = random.choice(['UP', 'DOWN', 'LEFT', 'RIGHT'])\n",
        "#         self.food = self.create_food()\n",
        "#         self.score = 0\n",
        "\n",
        "#     def create_food(self):\n",
        "#         while True:\n",
        "#             x = random.randint(0, self.width - 1)\n",
        "#             y = random.randint(0, self.height - 1)\n",
        "#             if (x, y) not in self.snake:\n",
        "#                 return x, y\n",
        "\n",
        "#     def move(self, action):\n",
        "#         # Map action index to direction\n",
        "#         directions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
        "#         self.direction = directions[action]\n",
        "\n",
        "#         head = self.snake[0]\n",
        "#         new_head = (head[0] + (self.direction == 'RIGHT') - (self.direction == 'LEFT'),\n",
        "#                     head[1] + (self.direction == 'DOWN') - (self.direction == 'UP'))\n",
        "\n",
        "#         if (new_head[0] < 0 or new_head[0] >= self.width or\n",
        "#                 new_head[1] < 0 or new_head[1] >= self.height or\n",
        "#                 new_head in self.snake):\n",
        "#             return -10, True  # Penalty for hitting wall or self\n",
        "\n",
        "#         reward = 0\n",
        "#         if new_head == self.food:\n",
        "#             self.food = self.create_food()\n",
        "#             reward = 1  # Reward for eating food\n",
        "#         else:\n",
        "#             self.snake.pop()\n",
        "\n",
        "#         self.snake.insert(0, new_head)\n",
        "#         return reward, False\n",
        "\n",
        "#     def get_state(self):\n",
        "#         state = np.zeros((self.width, self.height))\n",
        "#         for x, y in self.snake:\n",
        "#             state[x, y] = 1\n",
        "#         state[self.food[0], self.food[1]] = 2\n",
        "#         return state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DQNAgent Class:\n",
        "\n",
        "- This class implements the DQN agent.\n",
        "- It initializes the agent's memory buffer, discount factor (gamma), exploration rate (epsilon), and the Q-network model.\n",
        "- The act method selects actions based on an epsilon-greedy policy.\n",
        "- The remember method stores experiences (state, action, reward, next_state, done) in the agent's memory buffer.\n",
        "- The replay method samples experiences from memory and performs the Q-learning update step.\n",
        "- The train method trains the agent by interacting with the game environment for a specified number of episodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4fndPSKFj9bn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# class DQNAgent:\n",
        "#     def __init__(self, state_size, action_size):\n",
        "#         self.state_size = state_size\n",
        "#         self.action_size = action_size\n",
        "#         self.memory = deque(maxlen=2000)\n",
        "#         self.gamma = 0.95 # The discount factor used in the calculation of the temporal difference target. It determines the importance of future rewards compared to immediate rewards.\n",
        "#         self.epsilon = 1.0 # The exploration rate, which controls the balance between exploration (taking random actions) and exploitation (taking actions based on learned Q-values). It starts at 1.0 and decays over time.\n",
        "#         self.epsilon_min = 0.01\n",
        "#         self.epsilon_decay = 0.995\n",
        "#         self.learning_rate = 0.001\n",
        "#         self.model = self.build_model()\n",
        "\n",
        "#     def build_model(self):\n",
        "#         model = Sequential()\n",
        "#         model.add(Dense(24, input_dim=self.state_size, activation='relu'))  # Use the full state_size tuple\n",
        "#         model.add(Dense(24, activation='relu'))\n",
        "#         model.add(Dense(self.action_size, activation='linear'))\n",
        "#         model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
        "#         return model\n",
        "\n",
        "#     # stores experiences (state, action, reward, next_state, done) in the agent's memory buffer.\n",
        "#     def remember(self, state, action, reward, next_state, done):\n",
        "#         self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "#     # selects actions based on an epsilon-greedy policy\n",
        "#     def act(self, state):\n",
        "#         if np.random.rand() <= self.epsilon:\n",
        "#             return random.randrange(self.action_size)\n",
        "#         act_values = self.model.predict(state)\n",
        "#         return np.argmax(act_values[0])  # Return action with highest Q-value\n",
        "\n",
        "#     # samples experiences from memory and performs the Q-learning update step.\n",
        "#     def replay(self, batch_size):\n",
        "#         if len(self.memory) < batch_size:\n",
        "#             return\n",
        "#         samples = random.sample(self.memory, batch_size)\n",
        "#         for state, action, reward, next_state, done in samples:\n",
        "#             target = self.model.predict(state)\n",
        "#             if not done:\n",
        "#                 target[0][action] = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
        "#             else:\n",
        "#                 target[0][action] = reward\n",
        "#             self.model.fit(state, target, epochs=1, verbose=0)\n",
        "\n",
        "#     def train(self, num_episodes, batch_size):\n",
        "#         for episode in range(num_episodes):\n",
        "#             game = SnakeGame()\n",
        "#             state = game.get_state()\n",
        "\n",
        "#             while True:\n",
        "#                 action = self.act(state)\n",
        "#                 reward, done = game.move(action)\n",
        "#                 next_state = game.get_state()\n",
        "#                 self.remember(state, action, reward, next_state, done)\n",
        "#                 state = next_state\n",
        "\n",
        "#                 if done:\n",
        "#                     print(f\"Episode: {episode}, Score: {game.score}\")\n",
        "#                     break\n",
        "\n",
        "#                 self.replay(batch_size)\n",
        "#                 self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "#         # Save the trained model (optional)\n",
        "#         self.model.save('snake_dqn_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yXe_w5tqkCHL"
      },
      "outputs": [],
      "source": [
        "# # Initialize and run the game and DQN agent\n",
        "# if __name__ == \"__main__\":\n",
        "#     state_size = (GRID_WIDTH, GRID_HEIGHT)\n",
        "#     action_size = 4  # Up, down, left, right\n",
        "#     agent = DQNAgent(state_size[0], action_size)\n",
        "#     agent.train(1, 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Game over! You collided with the snake.\n",
            "Game over! You collided with the snake.\n",
            "Game over! You collided with the snake.\n",
            "Game over! You collided with the snake.\n",
            "Game over! You collided with the snake.\n",
            "Game over! You collided with the snake.\n",
            "Game over! You collided with the snake.\n",
            "Game over! You collided with the snake.\n",
            "Game over! You collided with the snake.\n",
            "Game over! You collided with the snake.\n",
            "Game over! You collided with the snake.\n",
            "Game over! You collided with the snake.\n",
            "Game over! You collided with the snake.\n",
            "Game over! You collided with the snake.\n",
            "Game over! You collided with the snake.\n",
            "Game over! You collided with the snake.\n",
            "Game over! You collided with the snake.\n",
            "Game over! You collided with the snake.\n",
            "Game over! You collided with the snake.\n",
            "Game over! You collided with the snake.\n",
            "Game over! You collided with the snake.\n",
            "Game over! You collided with the snake.\n",
            "Game over! You collided with the snake.\n",
            "Game over! You collided with the snake.\n"
          ]
        }
      ],
      "source": [
        "import tkinter as tk\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "import random\n",
        "\n",
        "class SnakeGUI(tk.Tk):\n",
        "    def __init__(self, grid_size=5):\n",
        "        super().__init__()\n",
        "        self.title(\"Snake Game\")\n",
        "        self.grid_size = grid_size\n",
        "        self.cell_size = 30\n",
        "        self.canvas = tk.Canvas(self, width=self.grid_size * self.cell_size, height=self.grid_size * self.cell_size)\n",
        "        self.canvas.pack()\n",
        "        self.env = SnakeEnv(grid_size)\n",
        "        self.reset()\n",
        "        self.render()\n",
        "\n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "\n",
        "    def render(self):\n",
        "        self.canvas.delete(\"all\")\n",
        "        for i in range(self.grid_size):\n",
        "            for j in range(self.grid_size):\n",
        "                x0, y0 = j * self.cell_size, i * self.cell_size\n",
        "                x1, y1 = x0 + self.cell_size, y0 + self.cell_size\n",
        "                if (i, j) in self.env.snake:\n",
        "                    self.canvas.create_rectangle(x0, y0, x1, y1, fill=\"green\")\n",
        "                elif (i, j) == self.env.food:\n",
        "                    self.canvas.create_rectangle(x0, y0, x1, y1, fill=\"red\")\n",
        "                else:\n",
        "                    self.canvas.create_rectangle(x0, y0, x1, y1, fill=\"white\")\n",
        "        self.canvas.after(500, self.step)\n",
        "\n",
        "    def step(self):\n",
        "        action = self.get_action()\n",
        "        state, reward, done, _ = self.env.step(action)\n",
        "        self.render()\n",
        "        if done:\n",
        "            if reward == 1.0:\n",
        "                print(\"Congratulations! You ate the food!\")\n",
        "            else:\n",
        "                print(\"Game over! You collided with the snake.\")\n",
        "\n",
        "    def get_action(self):\n",
        "        # Implement  action selection logic here, for example, random action for demonstration\n",
        "        return random.randint(0, 3)\n",
        "\n",
        "class SnakeEnv(gym.Env):\n",
        "    def __init__(self, grid_size=5):\n",
        "        self.grid_size = grid_size\n",
        "        self.observation_space = spaces.Box(low=0, high=1, shape=(self.grid_size, self.grid_size), dtype=np.float32)\n",
        "        self.action_space = spaces.Discrete(4)  # 4 possible actions: move left, right, up, down\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.snake = [(0, 0)]  # Reset Snake's position\n",
        "        self.food = self._generate_food()  # Reset food position\n",
        "        return self._get_observation()\n",
        "\n",
        "    def step(self, action):\n",
        "        head = self.snake[0]\n",
        "        new_head = self._get_new_head(head, action)\n",
        "\n",
        "        # Check for collisions\n",
        "        if new_head in self.snake or not self._is_inside_grid(new_head):\n",
        "            done = True\n",
        "            reward = -1.0\n",
        "        elif new_head == self.food:\n",
        "            self.snake.insert(0, new_head)\n",
        "            self.food = self._generate_food()\n",
        "            reward = 1.0\n",
        "            done = False\n",
        "        else:\n",
        "            self.snake.insert(0, new_head)\n",
        "            self.snake.pop()\n",
        "            reward = 0.0\n",
        "            done = False\n",
        "\n",
        "        return self._get_observation(), reward, done, {}\n",
        "\n",
        "    def _get_observation(self):\n",
        "        observation = np.zeros((self.grid_size, self.grid_size), dtype=np.float32)\n",
        "        for segment in self.snake:\n",
        "            observation[segment[0], segment[1]] = 1.0\n",
        "        observation[self.food[0], self.food[1]] = 0.5\n",
        "        return observation\n",
        "\n",
        "    def _get_new_head(self, head, action):\n",
        "        x, y = head\n",
        "        if action == 0:  # Move left\n",
        "            y -= 1\n",
        "        elif action == 1:  # Move right\n",
        "            y += 1\n",
        "        elif action == 2:  # Move up\n",
        "            x -= 1\n",
        "        elif action == 3:  # Move down\n",
        "            x += 1\n",
        "        return x, y\n",
        "\n",
        "    def _is_inside_grid(self, position):\n",
        "        x, y = position\n",
        "        return 0 <= x < self.grid_size and 0 <= y < self.grid_size\n",
        "\n",
        "    def _generate_food(self):\n",
        "        empty_cells = [(i, j) for i in range(self.grid_size) for j in range(self.grid_size) if (i, j) not in self.snake]\n",
        "        return random.choice(empty_cells)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app = SnakeGUI(grid_size=10)\n",
        "    app.mainloop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed.\n"
          ]
        },
        {
          "ename": "NotImplementedError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 113>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    114\u001b[0m     train_snake()  \u001b[38;5;66;03m# Train the snake\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[43mplay_snake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mplay_snake\u001b[1;34m()\u001b[0m\n\u001b[0;32m     93\u001b[0m env \u001b[38;5;241m=\u001b[39m SnakeEnv(grid_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     94\u001b[0m state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m---> 95\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter action (0: left, 1: right, 2: up, 3: down, q: quit): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\alok\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\core.py:176\u001b[0m, in \u001b[0;36mEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the render frames as specified by render_mode attribute during initialization of the environment.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m    The set of supported modes varies per environment. (And some\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m        in implementations to use the functionality of this method.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
            "\u001b[1;31mNotImplementedError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# import numpy as np\n",
        "# import gym\n",
        "# from gym import spaces\n",
        "# import random\n",
        "\n",
        "# class SnakeEnv(gym.Env):\n",
        "#     def __init__(self, grid_size=5):\n",
        "#         self.grid_size = grid_size\n",
        "#         self.observation_space = spaces.Box(low=0, high=1, shape=(self.grid_size, self.grid_size), dtype=np.float32)\n",
        "#         self.action_space = spaces.Discrete(4)  # 4 possible actions: move left, right, up, down\n",
        "#         self.q_table = np.zeros((self.grid_size, self.grid_size, 4))  # Q-table for state-action values\n",
        "#         self.reset()\n",
        "\n",
        "#     def reset(self):\n",
        "#         self.snake = [(0, 0)]  # Reset Snake's position\n",
        "#         self.food = self._generate_food()  # Reset food position\n",
        "#         return self._get_observation()\n",
        "\n",
        "#     def step(self, action):\n",
        "#         head = self.snake[0]\n",
        "#         new_head = self._get_new_head(head, action)\n",
        "\n",
        "#         # Check for collisions\n",
        "#         if new_head in self.snake or not self._is_inside_grid(new_head):\n",
        "#             done = True\n",
        "#             reward = -1.0\n",
        "#         elif new_head == self.food:\n",
        "#             self.snake.insert(0, new_head)\n",
        "#             self.food = self._generate_food()\n",
        "#             reward = 1.0\n",
        "#             done = False\n",
        "#         else:\n",
        "#             self.snake.insert(0, new_head)\n",
        "#             self.snake.pop()\n",
        "#             reward = 0.0\n",
        "#             done = False\n",
        "\n",
        "#         # Update Q-value for the previous state-action pair\n",
        "#         prev_state = self._get_observation()\n",
        "#         self.q_table[prev_state[0], prev_state[1], action] += reward\n",
        "\n",
        "#         return self._get_observation(), reward, done, {}\n",
        "\n",
        "#     def _get_observation(self):\n",
        "#         head = self.snake[0]\n",
        "#         return head[0], head[1]\n",
        "\n",
        "#     def _get_new_head(self, head, action):\n",
        "#         x, y = head\n",
        "#         if action == 0:  # Move left\n",
        "#             y -= 1\n",
        "#         elif action == 1:  # Move right\n",
        "#             y += 1\n",
        "#         elif action == 2:  # Move up\n",
        "#             x -= 1\n",
        "#         elif action == 3:  # Move down\n",
        "#             x += 1\n",
        "#         return x, y\n",
        "\n",
        "#     def _is_inside_grid(self, position):\n",
        "#         x, y = position\n",
        "#         return 0 <= x < self.grid_size and 0 <= y < self.grid_size\n",
        "\n",
        "#     def _generate_food(self):\n",
        "#         empty_cells = [(i, j) for i in range(self.grid_size) for j in range(self.grid_size) if (i, j) not in self.snake]\n",
        "#         return random.choice(empty_cells)\n",
        "\n",
        "#     def get_action(self):\n",
        "#         # Epsilon-greedy policy for action selection\n",
        "#         epsilon = 0.1\n",
        "#         if random.random() < epsilon:\n",
        "#             return self.action_space.sample()  # Random action\n",
        "#         else:\n",
        "#             head = self.snake[0]\n",
        "#             state = self._get_observation()\n",
        "#             return np.argmax(self.q_table[state[0], state[1], :])  # Greedy action selection\n",
        "\n",
        "# # Main function for training\n",
        "# def train_snake():\n",
        "#     env = SnakeEnv(grid_size=5)\n",
        "#     num_episodes = 1000\n",
        "#     for episode in range(num_episodes):\n",
        "#         state = env.reset()\n",
        "#         done = False\n",
        "#         while not done:\n",
        "#             action = env.get_action()\n",
        "#             next_state, reward, done, _ = env.step(action)\n",
        "#             # Training is done online, so no explicit training step is needed\n",
        "#     print(\"Training completed.\")\n",
        "\n",
        "# # Main function for playing after training\n",
        "# def play_snake():\n",
        "#     env = SnakeEnv(grid_size=5)\n",
        "#     state = env.reset()\n",
        "#     env.render()\n",
        "#     while True:\n",
        "#         action = input(\"Enter action (0: left, 1: right, 2: up, 3: down, q: quit): \")\n",
        "#         if action == 'q':\n",
        "#             print(\"Quitting game...\")\n",
        "#             break\n",
        "#         elif action.isdigit() and int(action) in range(4):\n",
        "#             state, reward, done, _ = env.step(int(action))\n",
        "#             env.render()\n",
        "#             if done:\n",
        "#                 if reward == 1.0:\n",
        "#                     print(\"Congratulations! You ate the food!\")\n",
        "#                 else:\n",
        "#                     print(\"Game over! You collided with the snake.\")\n",
        "#                 break\n",
        "#         else:\n",
        "#             print(\"Invalid action. Please enter a valid action.\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     train_snake()  # Train the snake\n",
        "#     play_snake()   # Play the game with the trained snake\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
